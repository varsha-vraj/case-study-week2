<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Ecommerce Data Vault ğŸš€</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #fdfdfd;
      padding: 2em;
      line-height: 1.6;
    }
    h1 {
      color: #2e86de;
    }
    h2 {
      color: #27ae60;
    }
    code {
      background: #f4f4f4;
      padding: 2px 4px;
      border-radius: 4px;
    }
    pre {
      background: #f4f4f4;
      padding: 10px;
      overflow-x: auto;
      border-radius: 6px;
    }
    img {
      max-width: 100%;
      border-radius: 8px;
      margin: 10px 0;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
  </style>
</head>
<body>

  <h1>ğŸ“¦ Ecommerce Data Vault Project ğŸš€</h1>

  <p>
    This project showcases a complete end-to-end data warehouse architecture using <strong>Data Vault 2.0</strong>, 
    <strong>Delta Lake</strong>, and <strong>PySpark</strong>. Built as part of a real-world case study to transform raw ecommerce data into a scalable, auditable, and analytics-ready platform.
  </p>

  <h2>ğŸ§° Tech Stack</h2>
  <ul>
    <li>ğŸ”¥ Apache Spark (PySpark)</li>
    <li>ğŸ§Š Delta Lake for ACID + versioning</li>
    <li>ğŸ Python with virtualenv</li>
    <li>ğŸ“ Local CSVs as data source</li>
    <li>ğŸ§ª Delta JARs for local Spark execution</li>
    <li>ğŸ“Š Designed for BI tools (Power BI, Tableau)</li>
  </ul>

  <h2>ğŸ“ Project Structure</h2>
  <pre>
ecommerce-data-vault/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ products.csv
â”‚   â””â”€â”€ orders.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ hub_*.py, link_*.py, sat_*.py
â”‚   â”œâ”€â”€ scd_type2_*.py, pit_tables.py
â”‚   â”œâ”€â”€ dim_customer.py, dim_product.py, fact_order.py
â”œâ”€â”€ delta_tables/
â”‚   â”œâ”€â”€ hub_*, sat_*, link_*, scd_*, pit_*, dim_*, fact_*
â””â”€â”€ README.html
  </pre>

  <h2>ğŸ§  Workflow Overview</h2>
  <ol>
    <li>ğŸ”½ Load raw CSV data from the <code>data/</code> folder.</li>
    <li>ğŸ—ï¸ Create Data Vault model using Hubs, Links, Satellites.</li>
    <li>ğŸ“œ Track history using SCD Type 2 with audit columns.</li>
    <li>ğŸ“Œ Generate PIT Tables to simplify latest joins.</li>
    <li>ğŸŒŸ Transform to Star Schema with surrogate keys.</li>
    <li>ğŸ“Š Connect to BI tools for insights.</li>
  </ol>

  <h2>ğŸ“Š ER Diagrams</h2>
  <p><strong>Data Vault ERD:</strong></p>
  <img src="https://raw.githubusercontent.com/your-username/your-repo/main/docs/datavault_erd.png" alt="Data Vault ERD">

  <p><strong>Star Schema ERD:</strong></p>
  <img src="https://raw.githubusercontent.com/your-username/your-repo/main/docs/star_erd.png" alt="Star Schema ERD">

  <h2>ğŸ Final Tables</h2>
  <ul>
    <li>ğŸ”· Hubs: <code>hub_customer</code>, <code>hub_product</code>, <code>hub_order</code></li>
    <li>ğŸ”— Links: <code>link_order_product</code>, <code>link_order_customer</code></li>
    <li>ğŸ“‘ Satellites: <code>sat_customer</code>, <code>sat_product</code>, <code>sat_order</code></li>
    <li>â³ SCD Type 2: <code>scd_sat_customer</code>, <code>scd_sat_product</code>, <code>scd_sat_order</code></li>
    <li>ğŸ“Œ PIT: <code>pit_customer</code>, <code>pit_product</code>, <code>pit_order</code></li>
    <li>â­ Star Schema: <code>dim_customer</code>, <code>dim_product</code>, <code>fact_order</code></li>
  </ul>

  <h2>âœ… Example Spark Submit Command</h2>
  <pre>
spark-submit ^
  --jars delta-core_2.12-2.4.0.jar,delta-storage-2.4.0.jar ^
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" ^
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" ^
  --conf spark.pyspark.python="venv/Scripts/python.exe" ^
  src/dim_customer.py
  </pre>

  <h2>ğŸ‘©â€ğŸ’» Author</h2>
  <p>
    <br>Email: <a href="mailto:varshaa112003@gmail.com">varshaa112003@gmail.com</a>
  </p>

</body>
</html>
